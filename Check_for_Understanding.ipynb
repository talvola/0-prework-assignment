{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209c5b2c",
   "metadata": {},
   "source": [
    "# Check for Understanding — Autograded (PyTorch)\n",
    "Run each cell. **Do not delete the asserts.**\n",
    "\n",
    "**Passing condition:** all asserts pass.\n",
    "\n",
    "Tip: If an assert fails, read its message, fix your code, and rerun the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427e2e53-e9d2-45b6-81cd-b23aac38c5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/site-packages (2.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.11/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.11/site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.11/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.11/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.11/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d45ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def _is_close(a, b, tol=1e-5):\n",
    "    return torch.allclose(a, b, atol=tol, rtol=0)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b096880",
   "metadata": {},
   "source": [
    "## Part 1 — Tensors & Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2508965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=\n",
      " tensor([[ 0.3367,  0.1288,  0.2345],\n",
      "        [ 0.2303, -1.1229, -0.1863]])\n",
      "shape: torch.Size([2, 3])\n",
      "dtype: torch.float32\n",
      "mean: tensor(-0.0631)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Tensor basics\n",
    "# TODO:\n",
    "# 1) Create a 2x3 tensor of random values called X\n",
    "# 2) Print X, X.shape, X.dtype\n",
    "# 3) Compute the mean of all elements and store it in x_mean (a 0-d tensor)\n",
    "\n",
    "X = torch.randn(2, 3)  # YOUR CODE HERE\n",
    "x_mean = X.mean()  # YOUR CODE HERE\n",
    "\n",
    "# Print statements (uncomment after implementing)\n",
    "print(\"X=\\n\", X)\n",
    "print(\"shape:\", X.shape)\n",
    "print(\"dtype:\", X.dtype)\n",
    "print(\"mean:\", x_mean)\n",
    "\n",
    "# --- autograder asserts (do not delete) ---\n",
    "assert isinstance(X, torch.Tensor), \"X must be a torch.Tensor\"\n",
    "assert X.shape == (2, 3), f\"X must have shape (2,3), got {tuple(X.shape)}\"\n",
    "assert X.dtype in (torch.float32, torch.float64), f\"X should be float32/float64, got {X.dtype}\"\n",
    "assert isinstance(x_mean, torch.Tensor) and x_mean.shape == (), \"x_mean must be a scalar (0-d) tensor\"\n",
    "assert _is_close(x_mean, X.sum() / X.numel()), \"x_mean should equal X.sum()/X.numel()\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9da944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1: tensor([ 2.2082, -0.6380,  0.4617,  0.2674,  0.5349])\n",
      "v2: tensor([ 0.8094,  1.1103, -1.6898, -0.9890,  0.9580])\n",
      "v_sum: tensor([ 3.0176,  0.4723, -1.2281, -0.7216,  1.4929])\n",
      "v_dot: tensor(0.5468)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: Manual vector operations\n",
    "# TODO:\n",
    "# 1) Create v1 and v2 as 1-D tensors of length 5\n",
    "# 2) Compute element-wise sum: v_sum\n",
    "# 3) Compute dot product: v_dot (scalar tensor)\n",
    "\n",
    "v1 = torch.randn(5)  # YOUR CODE HERE\n",
    "v2 = torch.randn(5)  # YOUR CODE HERE\n",
    "\n",
    "v_sum = v1 + v2  # YOUR CODE HERE\n",
    "v_dot = torch.dot(v1, v2)  # YOUR CODE HERE\n",
    "\n",
    "# Print statements (uncomment after implementing)\n",
    "print(\"v1:\", v1)\n",
    "print(\"v2:\", v2)\n",
    "print(\"v_sum:\", v_sum)\n",
    "print(\"v_dot:\", v_dot)\n",
    "\n",
    "# --- autograder asserts (do not delete) ---\n",
    "assert v1.shape == (5,) and v2.shape == (5,), \"v1 and v2 must both be shape (5,)\"\n",
    "assert v_sum.shape == (5,), \"v_sum must be a length-5 vector\"\n",
    "assert v_dot.shape == (), \"v_dot must be a scalar (0-d) tensor\"\n",
    "manual_dot = (v1 * v2).sum()\n",
    "assert _is_close(v_dot, manual_dot), \"v_dot must equal (v1*v2).sum()\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55cc96",
   "metadata": {},
   "source": [
    "## Part 2 — Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca24709c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids: tensor([2, 5, 7])\n",
      "E=\n",
      " tensor([[-1.5576,  0.9956, -0.8798, -0.6011],\n",
      "        [-0.4880,  1.1914, -0.8140, -0.7360],\n",
      "        [ 0.1971, -1.1441,  0.3383,  1.6992]], grad_fn=<EmbeddingBackward0>)\n",
      "E.shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Simple embedding lookup\n",
    "# TODO:\n",
    "# 1) Create an nn.Embedding called emb with vocab_size=10 and emb_dim=4\n",
    "# 2) Create token_ids as a LongTensor of shape (3,) with values in [0, 9]\n",
    "# 3) Lookup embeddings: E = emb(token_ids)\n",
    "# 4) Print E and E.shape\n",
    "\n",
    "vocab_size, emb_dim = 10, 4\n",
    "emb = nn.Embedding(vocab_size, emb_dim)  # YOUR CODE HERE\n",
    "\n",
    "token_ids = torch.LongTensor([2, 5, 7])  # YOUR CODE HERE\n",
    "E = emb(token_ids)  # YOUR CODE HERE\n",
    "\n",
    "# Print statements (uncomment after implementing)\n",
    "print(\"token_ids:\", token_ids)\n",
    "print(\"E=\\n\", E)\n",
    "print(\"E.shape:\", E.shape)\n",
    "\n",
    "# --- autograder asserts (do not delete) ---\n",
    "assert isinstance(emb, nn.Embedding), \"emb must be an nn.Embedding\"\n",
    "assert token_ids.dtype == torch.long, \"token_ids must be torch.long\"\n",
    "assert token_ids.shape == (3,), f\"token_ids must be shape (3,), got {tuple(token_ids.shape)}\"\n",
    "assert E.shape == (3, 4), f\"E must have shape (3,4), got {tuple(E.shape)}\"\n",
    "assert E.requires_grad, \"Embedding output should require gradients by default\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e75f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_E.shape: torch.Size([4])\n",
      "y_pred: tensor([0.4434], grad_fn=<ViewBackward0>) shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: From embeddings to a prediction\n",
    "# NOTE: This exercise depends on Exercise 3 — complete that first.\n",
    "# TODO:\n",
    "# 1) Compute mean embedding across tokens: mean_E of shape (4,)\n",
    "# 2) Create a Linear layer (4 -> 1) called head\n",
    "# 3) Produce y_pred as shape (1,) or scalar\n",
    "\n",
    "mean_E = E.mean(dim=0)  # YOUR CODE HERE\n",
    "head = nn.Linear(in_features=4, out_features=1)  # YOUR CODE HERE\n",
    "\n",
    "y_pred = head(mean_E)  # YOUR CODE HERE\n",
    "\n",
    "# Print statements (uncomment after implementing)\n",
    "print(\"mean_E.shape:\", mean_E.shape)\n",
    "print(\"y_pred:\", y_pred, \"shape:\", y_pred.shape)\n",
    "\n",
    "# --- autograder asserts (do not delete) ---\n",
    "assert mean_E.shape == (4,), f\"mean_E must be shape (4,), got {tuple(mean_E.shape)}\"\n",
    "assert isinstance(head, nn.Linear) and head.in_features == 4 and head.out_features == 1, \"head must be Linear(4->1)\"\n",
    "assert y_pred.numel() == 1, \"y_pred must have exactly 1 element\"\n",
    "assert y_pred.requires_grad, \"y_pred should require gradients\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e09a31",
   "metadata": {},
   "source": [
    "## Part 3 — Build a Tiny Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "947292e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (fc1): Linear(in_features=6, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5: Define a simple feed-forward network\n",
    "# Requirements:\n",
    "# - input_dim = 6\n",
    "# - hidden_dim = 8\n",
    "# - output_dim = 1\n",
    "# - 1 hidden layer + ReLU\n",
    "# Implement SimpleNet so forward(x) returns shape (batch, 1)\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=8, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNet()\n",
    "print(model)\n",
    "\n",
    "# --- autograder asserts (do not delete) ---\n",
    "assert isinstance(model, nn.Module), \"model must be an nn.Module\"\n",
    "params = dict(model.named_parameters())\n",
    "assert \"fc1.weight\" in params and \"fc2.weight\" in params, \"Model must have two Linear layers (fc1, fc2)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d349030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out=\n",
      " tensor([[-0.2932],\n",
      "        [-0.1127],\n",
      "        [-0.3927],\n",
      "        [-0.2312]], grad_fn=<AddmmBackward0>)\n",
      "out.shape: torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6: Forward pass with dummy data\n",
    "# NOTE: This exercise depends on Exercise 5 — complete that first.\n",
    "# TODO:\n",
    "# 1) Create dummy input x of shape (4, 6)\n",
    "# 2) Run out = model(x)\n",
    "# 3) Print out and out.shape\n",
    "\n",
    "x = torch.randn(4, 6)  # YOUR CODE HERE\n",
    "out = model(x)  # YOUR CODE HERE\n",
    "\n",
    "# Print statements (uncomment after implementing)\n",
    "print(\"out=\\n\", out)\n",
    "print(\"out.shape:\", out.shape)\n",
    "\n",
    "# --- autograder asserts (do not delete) ---\n",
    "assert x.shape == (4, 6), f\"x must be shape (4,6), got {tuple(x.shape)}\"\n",
    "assert out.shape == (4, 1), f\"out must be shape (4,1), got {tuple(out.shape)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db1cca",
   "metadata": {},
   "source": [
    "## Part 4 — One Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60eaed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_before: 1.0137972831726074\n",
      "loss_after : 0.7375679016113281\n"
     ]
    }
   ],
   "source": [
    "# Exercise 7: One training step\n",
    "# NOTE: This exercise depends on Exercise 5 — complete that first.\n",
    "# TODO:\n",
    "# 1) Create inputs x_train (batch=8, input_dim=6) and targets y_train (shape (8,1))\n",
    "# 2) Define loss_fn = MSELoss and opt = SGD(model.parameters(), lr=0.1)\n",
    "# 3) Perform exactly one update step and print loss_before and loss_after\n",
    "\n",
    "torch.manual_seed(123)  # deterministic for this part\n",
    "\n",
    "# Create training data (provided for you)\n",
    "x_train = torch.randn(8, 6)\n",
    "true_w = torch.tensor([[0.5], [-1.0], [0.3], [0.0], [1.2], [-0.7]])\n",
    "y_train = x_train @ true_w + 0.01 * torch.randn(8, 1)\n",
    "\n",
    "loss_fn = nn.MSELoss()  # YOUR CODE HERE\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)  # YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE — compute loss_before (forward pass + loss)\n",
    "loss_before = loss_fn(model(x_train), y_train)\n",
    "\n",
    "# YOUR CODE HERE — perform backward pass and optimizer step\n",
    "opt.zero_grad()       # Clear old gradients\n",
    "loss_before.backward() # Compute gradients\n",
    "opt.step()            # Update weights\n",
    "\n",
    "# YOUR CODE HERE — compute loss_after (forward pass + loss)\n",
    "loss_after = loss_fn(model(x_train), y_train)\n",
    "\n",
    "# Print statements (uncomment after implementing)\n",
    "print(\"loss_before:\", float(loss_before))\n",
    "print(\"loss_after :\", float(loss_after))\n",
    "\n",
    "# --- autograder asserts (do not delete) ---\n",
    "assert loss_before.shape == (), \"loss_before must be a scalar tensor\"\n",
    "assert loss_after.shape == (), \"loss_after must be a scalar tensor\"\n",
    "assert float(loss_after) < float(loss_before), \"loss_after should be < loss_before after one SGD step\"\n",
    "assert loss_after.detach() < loss_before.detach(), \"loss_after should be < loss_before after one SGD step\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e888138",
   "metadata": {},
   "source": [
    "## Optional Stretch (No grade)\n",
    "If you finish early:\n",
    "1. Add a second training step and show loss keeps decreasing.\n",
    "2. Change activation to Tanh and compare loss curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed8b397a-bd51-4f55-941d-9fb0d6926d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step | ReLU Loss | Tanh Loss\n",
      "------------------------------\n",
      "   1 | 1.3535    | 1.2773\n",
      "   5 | 0.9495    | 0.6275\n",
      "  10 | 0.6613    | 0.2862\n",
      "  15 | 0.4172    | 0.1357\n",
      "  20 | 0.2336    | 0.0753\n"
     ]
    }
   ],
   "source": [
    "  # Define TanhNet (same architecture, different activation)\n",
    "  class TanhNet(nn.Module):\n",
    "      def __init__(self, input_dim=6, hidden_dim=8, output_dim=1):\n",
    "          super().__init__()\n",
    "          self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "          self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "          self.activation = nn.Tanh()  # <-- Changed from ReLU\n",
    "\n",
    "      def forward(self, x):\n",
    "          x = self.fc1(x)\n",
    "          x = self.activation(x)\n",
    "          x = self.fc2(x)\n",
    "          return x\n",
    "\n",
    "  # Train both models for multiple steps and compare\n",
    "  torch.manual_seed(123)\n",
    "  x_train = torch.randn(8, 6)\n",
    "  true_w = torch.tensor([[0.5], [-1.0], [0.3], [0.0], [1.2], [-0.7]])\n",
    "  y_train = x_train @ true_w + 0.01 * torch.randn(8, 1)\n",
    "\n",
    "  torch.manual_seed(42)  # Same initialization for fair comparison\n",
    "  relu_model = SimpleNet()\n",
    "  tanh_model = TanhNet()\n",
    "\n",
    "  # Copy same initial weights to tanh_model for fair comparison\n",
    "  tanh_model.load_state_dict(relu_model.state_dict())\n",
    "\n",
    "  loss_fn = nn.MSELoss()\n",
    "  relu_opt = torch.optim.SGD(relu_model.parameters(), lr=0.1)\n",
    "  tanh_opt = torch.optim.SGD(tanh_model.parameters(), lr=0.1)\n",
    "\n",
    "  relu_losses = []\n",
    "  tanh_losses = []\n",
    "\n",
    "  # Train for 20 steps\n",
    "  for step in range(20):\n",
    "      # ReLU model\n",
    "      relu_opt.zero_grad()\n",
    "      relu_loss = loss_fn(relu_model(x_train), y_train)\n",
    "      relu_loss.backward()\n",
    "      relu_opt.step()\n",
    "      relu_losses.append(relu_loss.item())\n",
    "\n",
    "      # Tanh model\n",
    "      tanh_opt.zero_grad()\n",
    "      tanh_loss = loss_fn(tanh_model(x_train), y_train)\n",
    "      tanh_loss.backward()\n",
    "      tanh_opt.step()\n",
    "      tanh_losses.append(tanh_loss.item())\n",
    "\n",
    "  # Print comparison\n",
    "  print(\"Step | ReLU Loss | Tanh Loss\")\n",
    "  print(\"-\" * 30)\n",
    "  for i in [0, 4, 9, 14, 19]:\n",
    "      print(f\"{i+1:4d} | {relu_losses[i]:.4f}    | {tanh_losses[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6185f20a-02f9-4de6-8843-7dba60223918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
